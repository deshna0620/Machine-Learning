# Perceptron from Scratch using NumPy

This project demonstrates a basic implementation of a **Perceptron**, the simplest form of a neural network, using **NumPy**. It is trained to simulate the behavior of an **AND logic gate**.

## Files

- `05-07-2025.py` – Python script implementing and training the perceptron.
- `05-07-2025.png` – Output plot showing the decision boundary after training.

## What is a Perceptron?

A **Perceptron** is a type of linear binary classifier. It takes a vector of inputs, applies weights (and a bias), passes the result through an activation function (typically step function), and produces an output of 0 or 1.

It learns by adjusting weights based on errors using a simple learning rule:
```
weights += learning_rate * (target - prediction) * input
```

## Dataset: AND Gate

The script uses the truth table of the AND gate:
```
Input: [0, 0] -> Output: 0  
Input: [0, 1] -> Output: 0  
Input: [1, 0] -> Output: 0  
Input: [1, 1] -> Output: 1  
```

## How It Works

1. Initializes weights to zero (including bias).
2. For each training sample and epoch:
   - Predicts output using current weights.
   - Calculates error.
   - Updates weights using the Perceptron learning rule.

## Requirements

- Python 3.7+
- NumPy
- matplotlib

Install dependencies using:

```bash
pip install numpy matplotlib
```

## How to Run

Run the script using:

```bash
python 05-07-2025.py
```

The script will:
- Train the perceptron on AND gate inputs.
- Print predictions on the console.
- Save a decision boundary visualization to `05-07-2025.png`.

## Output

- Console: Prints the predicted output for each input.
- File: `05-07-2025.png` – A plot showing data points and the learned decision boundary.

## Notes

- The Perceptron can only solve linearly separable problems like AND or OR.
- This implementation does not work for XOR without adding layers (i.e., not a single-layer perceptron).
